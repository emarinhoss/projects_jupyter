{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Win a Data Science/Kaggle Competition\n",
    "\n",
    "* Yandex\n",
    "\n",
    "#### Real World ML Pipeline is a complicated process which includes:\n",
    "* Understanding the business problem\n",
    "* Problem formalization\n",
    "* Data collecting\n",
    "* Data preprocessing\n",
    "* Modeling\n",
    "* Way to evaluate model in real life\n",
    "* Way to deploy model\n",
    "\n",
    "#### It's not only about algorithms\n",
    "* It is all about data and making things work, not about algorithms themselves\n",
    "    * Everyone can and will tune classic approaches\n",
    "    * We need some insights to win\n",
    "* Sometimes there is no ML\n",
    "\n",
    "#### Do not limit yourself\n",
    "* It is ok to use heuristics, and manual data analysis\n",
    "* Do not be afraid of\n",
    "    * Complex solutions\n",
    "    * Advanced feature engineering\n",
    "    * Doing huge calculation\n",
    "    \n",
    "#### Be creative\n",
    "* It is OK to modify or hack existing algorithms or even to design a completly new algorithm\n",
    "* Do not be afraid of reading source codes and changing them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __VOWPAL WABBIT__: linear model used for large datasets\n",
    "* pyTorch\n",
    "\n",
    "[Playground](http://playground.tensorflow.org)\n",
    "\n",
    "[Classifier Comparison](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "* feature preprocessing is often necessary\n",
    "* feature generation (define a new fearure that might provide better insight)\n",
    "* preprocessing and generation pipelines depend on model type\n",
    "\n",
    "### Numeric features\n",
    "* scaling\n",
    "* rank, moves outliers closer to the mean\n",
    "* Log transform, and raising to the power <1, can help linear and NN by bringing features closer to the mean\n",
    "\n",
    "### Categorical and ordinal features\n",
    "* Frequency encoding maps categories to numbers\n",
    "* Label and frequency encodings are often used for tree-based models\n",
    "* One-hot encoding is often used for non-tree-based models\n",
    "* Interactions of categorical features can help linear models and KNN\n",
    "* adding two features in categorical data before label ecoding could be very helpful too.\n",
    "\n",
    "### Datetime and coordinates\n",
    "* Date and time\n",
    "    1. Periodicity, number of days, weeks, months, seasons, yeards, seconds, minutes, and hours\n",
    "    2. Time since, row-independent/dependent moment\n",
    "    3. Difference between dates\n",
    "* Coordinates\n",
    "    * Interesting places from train/tast data or additional data\n",
    "    * Centers of clusters\n",
    "    * Aggregated statistics\n",
    "    \n",
    "### Missing data\n",
    "* Fillna approaches\n",
    "    * -999, -1, etc\n",
    "    * mean, median\n",
    "    * Reconstruct value\n",
    "* The choice of method to fill NaN depends on the situation\n",
    "* Usual way to deal with missing values is to replace them with -999, mean or median\n",
    "* Binary feature **isnull** can be beneficial\n",
    "* In general, avoid filling nans before feature generation\n",
    "* Xgboost can handle NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction from texts and images\n",
    "\n",
    "#### Text to vector\n",
    "1. Bag of words, creates a arrays where earch column corresponds to a word and row to a sentence. If a word appears in a sentces that column gets a 1, all other columns get a 0.\n",
    "    * sklearn_feature_extraction.txt.CountVectorizer\n",
    "    * TFiDF\n",
    "        * term frequency, $tf=1/x.sum(axis=1)[:,None]; x=x*tf$\n",
    "        * Inverse Document Frequency, $idf=np.log(x.shape[0]/(x>0).sum(0)); x=x*idf$\n",
    "        * sklearn.feature_extraction.text.TfidVectorizer\n",
    "    * N-grams, sklearn_feature_extraction.txt.CountVectorizer, **Ngram_range** changes the overlap size, while **analyzer** is fo character Ngrams\n",
    "    * Preprocess text\n",
    "        * Lowercase\n",
    "        * Lemmatixation/Stemming: stemming cuts the words to a common origin, while lemmatization using morphology and origin of words\n",
    "        * Stopwods: prepositions and very common words\n",
    "    * Very large vectors\n",
    "    * Meaning of each value in vector is know\n",
    "2. Embedings (~word2vec): converts each word to a vector in a sophisticated space. Addition and substraction can be applied to this vectors and the result should be interpretable, e.g. king+woman-man=queen\n",
    "    * Words: Word2vec, Glove, FastText, etc\n",
    "    * Sentences: Doc2vec, etc\n",
    "    * Relatively small vectors\n",
    "    * Values in vector can be interpreted only in some cases\n",
    "    * The words with similar meaning often have similar embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to vector\n",
    "\n",
    "1. Descriptors\n",
    "2. Training network from scratch\n",
    "3. Finetuning\n",
    "4. Augmentation: rotating/stretching the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
