{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sousae/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, LSTM, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras.initializers import Ones\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from matplotlib import rcParams  # next 3 lines set font family for plotting\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.sans-serif'] = ['TImes New Roman']\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% SETTINGS FOR REPRODUCIBLE RESULTS DURING DEVELOPMENT\n",
    "\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "\n",
    "#import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in and preprocess data\n",
    "\n",
    "Xstatic = pd.read_csv('pulse2d_iterinput.txt', sep='\\t')  # read in static inputs (Dx, Dy, C0, etc...)\n",
    "wd = os.getcwd()  # set current directory as working directory path\n",
    "out_lst = [f for f in os.listdir(wd) if re.match('pulse2d_iteroutput[0-9]*.txt', f)]  # create list of filenames for output files of iterative runs\n",
    "data = pd.read_csv(out_lst[0], sep='\\t')  # read in the first output to get the shape\n",
    "Nsamples = 1000  # number of samples for all I/O is equal to the number of iterative runs\n",
    "Ntimesteps = data.shape[0]  # number of 'timesteps', in this case is equal to the number of domain nodes\n",
    "XNfeatures = data.shape[1]-1  # three output sequences will be used as input to the network to drive the LSTM\n",
    "XSNfeatures = Xstatic.shape[1]  # static features\n",
    "YNfeatures = 1  # only one prediction of interest....(Cmax)\n",
    "Xdim2D = (Nsamples,Ntimesteps*XNfeatures)  # dimension for 2D dynamic input\n",
    "Ydim2D = (Nsamples,Ntimesteps*YNfeatures)  # dimension for 2D output\n",
    "X2D = np.zeros(Xdim2D)  # initialize 2D dynamic input as numpy array of zeros\n",
    "Y2D = np.zeros(Ydim2D)  # initialize 2D dynamic output...\n",
    "Xdim3D = (Nsamples,Ntimesteps,XNfeatures)  # dimension for 3D dynamic input\n",
    "Ydim3D = (Nsamples,Ntimesteps,YNfeatures)  # dimension for 3D output\n",
    "X3D = np.zeros(Xdim3D)  # initialize 3D dynamic input as numpy array of zeros\n",
    "Y3D = np.zeros(Ydim3D)  # initialize 3D output....\n",
    "\n",
    "# build X and Y - read in dynamic I/O...i.e. output from the pulse2D model. The output has four predictions total, only one of which is the desired prediciton (Cmax = max concentration at domain node d). The remaining predictions will be used as input to the LSTM.\n",
    "for i,f in enumerate(out_lst):\n",
    "    data = pd.read_csv(f, sep='\\t')\n",
    "    X3D[i,:,:] = data.iloc[:,1:].values.reshape(X3D[i,:,:].shape)\n",
    "    Y3D[i,:,:] = data.Cmax.values.reshape(Y3D[i,:,:].shape)\n",
    "    \n",
    "# reshape 2D inputs  \n",
    "X2D = X3D.reshape(Xdim2D)  # same data, just prepped for input to an MLP\n",
    "X2D = np.concatenate((X2D,Xstatic),axis=1)  # concatenate the static data onto the dynamic data for MLP input\n",
    "Y2D = Y3D.reshape(Ydim2D)\n",
    "\n",
    "# normalize 2D inputs scross samples\n",
    "Xstatic = preprocessing.normalize(Xstatic,norm='l2',axis=1)\n",
    "X2D = preprocessing.normalize(X2D,norm='l2',axis=1)\n",
    "\n",
    "# normalize 3D inputs for each sample across time index for each feature\n",
    "for k in range(X3D.shape[2]):\n",
    "    X3D[:,:,k] = preprocessing.normalize(X3D[:,:,k],norm='l2',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle and partition train/test sets\n",
    "ismpls = list(i for i in range(0,Nsamples))  # create list of integers 1:Nsamples (row indices for all I/O)\n",
    "shuffle(ismpls)  # randomize sample/row indices \n",
    "ismpls = np.argsort(ismpls)  # I don't remember why this was important....\n",
    "\n",
    "# reorder each dataset using the randomized row indices set above. then, partition each dataset into train/test sets\n",
    "Xstatic = Xstatic[ismpls,:]\n",
    "XS_train = Xstatic[:900,:]\n",
    "XS_test = Xstatic[900:,:]\n",
    "\n",
    "X2D = X2D[ismpls,:]\n",
    "Y2D = Y2D[ismpls,:]\n",
    "X2D_train = X2D[:900,:]\n",
    "X2D_test = X2D[900:,:]\n",
    "Y2D_train = Y2D[:900,:]\n",
    "Y2D_test = Y2D[900:,:]\n",
    "\n",
    "X3D = X3D[ismpls,:,:]\n",
    "Y3D = Y3D[ismpls,:,:]\n",
    "X3D_train = X3D[:900,:,:]\n",
    "X3D_test = X3D[900:,:,:]\n",
    "Y3D_train = Y3D[:900,:,:]\n",
    "Y3D_test = Y3D[900:,:,:]\n",
    "\n",
    "# reshape the 3D outputs to calculate loss for LSTM and hybrid models\n",
    "Y3D_train = Y3D_train.reshape(Y3D_train.shape[0],Y3D_train.shape[1])\n",
    "Y3D_test = Y3D[900:,:,:].reshape(Y3D_test.shape[0],Y3D_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build MLP model\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "do = 0.2\n",
    "Nnodes = 64\n",
    "\n",
    "# create input layer..........\n",
    "main_input = Input(shape=(Ntimesteps*XNfeatures+XSNfeatures),\n",
    "                   dtype='float',\n",
    "                   batch_shape=(batch_size,Ntimesteps*XNfeatures+XSNfeatures),\n",
    "                   name='main_input'\n",
    "                   )\n",
    "\n",
    "#create hidden layer..........\n",
    "hidden_layer1 = Dense(Nnodes, activation='relu', name='hidden_layer1')(main_input)\n",
    "# add dropout to hidden layer\n",
    "Dropout(do)(hidden_layer1)\n",
    "hidden_layer1 = BatchNormalization()(hidden_layer1)\n",
    "\n",
    "# create output layer\n",
    "main_output = Dense(Ntimesteps*YNfeatures, name='main_output')(hidden_layer1)  # default activation is linear\n",
    "\n",
    "# feed datasets into model for training\n",
    "model = Model(inputs=[main_input], \n",
    "              outputs=[main_output]\n",
    "              )\n",
    "\n",
    "# compile the model with desired configuration\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['mae'])\n",
    "\n",
    "# one of several callbacks available in Keras, csv_logger saves metrics for every epoch to a csv file\n",
    "csv_logger = CSVLogger('trainingMLP_' + str(epochs) + '.log')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', # quantity to monitor\n",
    "                           min_delta=0.001,  # min change to qualify as an improvement\n",
    "                           patience=10, # stop after #epochs with no improvement\n",
    "                           verbose=1)  # print messages\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,  # reduction factor (new_lr = lr * factor)\n",
    "                              patience=5,\n",
    "                              verbose=1)\n",
    "\n",
    "# train the model, and store training information in the history object\n",
    "history = model.fit([X2D_train],[Y2D_train],\n",
    "                    epochs=epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(X2D_test, Y2D_test),\n",
    "                    callbacks=[csv_logger],\n",
    "#                    callbacks=[reduce_lr,early_stop,csv_logger]\n",
    "                    )\n",
    "\n",
    "histdict = history.history\n",
    "model.summary()  # print out a summary of layers/parameters\n",
    "config = model.get_config()  # detailed information about the configuration of each layer\n",
    "\n",
    "# evaluate the trained model on the test data set \n",
    "test = model.evaluate([X2D_test],[Y2D_test],batch_size=batch_size)  \n",
    "names = model.metrics_names\n",
    "\n",
    "predict = model.predict([X2D_test],batch_size=batch_size)\n",
    "Y_2Drmse = np.sqrt(mean_squared_error(predict,Y2D_test))\n",
    "print('Y_2Drmse:   ',Y_2Drmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot MLP output\n",
    "\n",
    "loss_train = histdict['loss']\n",
    "loss_test = histdict['val_loss']\n",
    "xplot = list(range(len(loss_train)))\n",
    "\n",
    "fig = plt.figure(num=1, figsize=(8,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "train = ax1.plot(xplot,np.sqrt(loss_train),'b-', label='Train', linewidth=4)\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Training Loss (RMSE)')\n",
    "for tl in ax1.get_yticklabels():\n",
    "    tl.set_color('b')\n",
    "ax2 = ax1.twinx()\n",
    "test = ax2.plot(xplot,np.sqrt(loss_test),'r-',label='Test',linewidth=4)\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Validation Loss (RMSE)')\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('r')\n",
    "curves = train + test\n",
    "labels = [c.get_label() for c in curves]\n",
    "ax1.legend(curves, labels, loc=0)\n",
    "plt.tight_layout()\n",
    "plt.title('MLP')\n",
    "plt.savefig('MLPout' + str(epochs) + 'epochs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build LSTM model\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "do = 0.2\n",
    "Nnodes = 64\n",
    "\n",
    "main_input = Input(shape=(Ntimesteps,XNfeatures),\n",
    "                   dtype='float',\n",
    "                   name='main_input'\n",
    "                   )\n",
    "\n",
    "# An LSTM will transform the vector sequence into a single vector,\n",
    "# containing information about the entire sequence\n",
    "\n",
    "lstm_out = LSTM(Nnodes,  # number of nodes in output dim \n",
    "                dropout=do,\n",
    "                recurrent_dropout=do, \n",
    "                use_bias=True, \n",
    "                unit_forget_bias=True,  # Jozefowicz et al\n",
    "                kernel_initializer=Ones(),  # Initializer for weight matrix acting on input \n",
    "                recurrent_initializer=Ones(),  # Initializer for hidden state weights\n",
    "                name='lstm_out'\n",
    "                )(main_input) \n",
    "main_output = Dense(Ntimesteps, name='main_output')(lstm_out)\n",
    "\n",
    "# feed datasets into model for training\n",
    "model = Model(inputs=[main_input], \n",
    "              outputs=[main_output]\n",
    "              )\n",
    "\n",
    "# compile the model with desired configuration\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['mae'])\n",
    "\n",
    "# one of several callbacks available in Keras, csv_logger saves metrics for every epoch to a csv file\n",
    "csv_logger = CSVLogger('trainingLSTM_' + str(epochs) + '.log')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', # quantity to monitor\n",
    "                           min_delta=0.001,  # min change to qualify as an improvement\n",
    "                           patience=10, # stop after #epochs with no improvement\n",
    "                           verbose=1)  # print messages\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,  # reduction factor (new_lr = lr * factor)\n",
    "                              patience=5,\n",
    "                              verbose=1)\n",
    "\n",
    "# train the model, and store training information in the history object\n",
    "history = model.fit([X3D_train],[Y3D_train],\n",
    "                    epochs=epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(X3D_test, Y3D_test),\n",
    "                    callbacks=[csv_logger],\n",
    "#                    callbacks=[reduce_lr,early_stop,csv_logger]\n",
    "                    )\n",
    "histdict = history.history \n",
    "model.summary()  # print out a summary of layers/parameters\n",
    "config = model.get_config()  # detailed information about the configuration of each layer\n",
    "\n",
    "# evaluate the trained model on the test data set\n",
    "test = model.evaluate([X3D_test],[Y3D_test],batch_size=batch_size)  \n",
    "names = model.metrics_names\n",
    "\n",
    "\n",
    "predict = model.predict([X3D_test],batch_size=batch_size)\n",
    "Y_3Drmse = np.sqrt(mean_squared_error(predict,Y3D_test))\n",
    "print('Y_3Drmse:   ',Y_3Drmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot LSTM output\n",
    "\n",
    "loss_train = histdict['loss']\n",
    "loss_test = histdict['val_loss']\n",
    "xplot = list(range(len(loss_train)))\n",
    "\n",
    "fig = plt.figure(num=2, figsize=(8,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "train = ax1.plot(xplot,np.sqrt(loss_train),'b-', label='Train', linewidth=4)\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Training Loss (RMSE)')\n",
    "for tl in ax1.get_yticklabels():\n",
    "    tl.set_color('b')\n",
    "ax2 = ax1.twinx()\n",
    "test = ax2.plot(xplot,np.sqrt(loss_test),'r-',label='Test',linewidth=4)\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Validation Loss (RMSE)')\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('r')\n",
    "curves = train + test\n",
    "labels = [c.get_label() for c in curves]\n",
    "ax1.legend(curves, labels, loc=0)\n",
    "plt.tight_layout()\n",
    "plt.title('LSTM')\n",
    "plt.savefig('LSTMout' + str(epochs) + 'epochs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build HYBRID model\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "do = 0.2\n",
    "Nnodes = 64\n",
    "\n",
    "main_input = Input(shape=(Ntimesteps,XNfeatures),\n",
    "                   dtype='float',\n",
    "                   name='main_input'\n",
    "                   )\n",
    "\n",
    "# An LSTM will transform the vector sequence into a single vector,\n",
    "# containing information about the entire sequence\n",
    "\n",
    "lstm_out = LSTM(XSNfeatures,  # number of nodes in output dim \n",
    "                dropout=do,\n",
    "                recurrent_dropout=do, \n",
    "                use_bias=True, \n",
    "                unit_forget_bias=True,  # Jozefowicz et al\n",
    "                kernel_initializer=Ones(),  # Initializer for weight matrix acting on input \n",
    "                recurrent_initializer=Ones(),  # Initializer for hidden state weights\n",
    "                name='lstm_out'\n",
    "                )(main_input) \n",
    "\n",
    "auxiliary_output = Dense(Ntimesteps, name='aux_output')(lstm_out)\n",
    "\n",
    "auxiliary_input = Input(shape=(XSNfeatures,), name='aux_input')  # here the shape argument specifies the dimension of each vector i.e. # features.  This needs to match the dimension of the static inputs (None, #static features)\n",
    "x = Concatenate(axis=-1)([lstm_out, auxiliary_input])  # concat on samples axis maintaining original shapes of both matrices\n",
    "# =============================================================================\n",
    "# x = Dense(Nnodes, activation='relu')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# Dropout(0.2)(x)\n",
    "# =============================================================================\n",
    "main_output = Dense(Ntimesteps, name='main_output')(x)\n",
    "\n",
    "# feed datasets into model for training\n",
    "model = Model(inputs=[main_input,auxiliary_input], \n",
    "              outputs=[main_output,auxiliary_output]\n",
    "              )\n",
    "\n",
    "# compile the model with desired configuration\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['mae'])\n",
    "\n",
    "# one of several callbacks available in Keras, csv_logger saves metrics for every epoch to a csv file\n",
    "csv_logger = CSVLogger('trainingLSTM_' + str(epochs) + '.log')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', # quantity to monitor\n",
    "                           min_delta=0.001,  # min change to qualify as an improvement\n",
    "                           patience=10, # stop after #epochs with no improvement\n",
    "                           verbose=1)  # print messages\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,  # reduction factor (new_lr = lr * factor)\n",
    "                              patience=5,\n",
    "                              verbose=1)\n",
    "\n",
    "# train the model, and store training information in the history object\n",
    "history = model.fit([X3D_train,XS_train],[Y3D_train,Y3D_train],\n",
    "                    epochs=epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=([X3D_test,XS_test],[Y3D_test,Y3D_test]),\n",
    "                    callbacks=[csv_logger],\n",
    "#                    callbacks=[reduce_lr,early_stop,csv_logger]\n",
    "                    )\n",
    "histdict = history.history \n",
    "model.summary()  # print out a summary of layers/parameters\n",
    "config = model.get_config()  # detailed information about the configuration of each layer\n",
    "\n",
    "# evaluate the trained model on the test data set\n",
    "test = model.evaluate([X3D_test,XS_test],[Y3D_test,Y3D_test],batch_size=batch_size)  \n",
    "names = model.metrics_names\n",
    "\n",
    "predict = model.predict([X3D_test,XS_test],batch_size=batch_size)\n",
    "predict = predict[0]\n",
    "Y_Hybridrmse = np.sqrt(mean_squared_error(predict,Y3D_test))\n",
    "print('Y_Hybridrmse:   ',Y_Hybridrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot HYBRID output\n",
    "\n",
    "# aux output\n",
    "loss_train = histdict['aux_output_loss']\n",
    "loss_test = histdict['val_aux_output_loss']\n",
    "xplot = list(range(len(loss_train)))\n",
    "\n",
    "fig = plt.figure(num=2, figsize=(8,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "train = ax1.plot(xplot,np.sqrt(loss_train),'b-', label='Train', linewidth=4)\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Training Loss (RMSE)')\n",
    "for tl in ax1.get_yticklabels():\n",
    "    tl.set_color('b')\n",
    "ax2 = ax1.twinx()\n",
    "test = ax2.plot(xplot,np.sqrt(loss_test),'r-',label='Test',linewidth=4)\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Validation Loss (RMSE)')\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('r')\n",
    "curves = train + test\n",
    "labels = [c.get_label() for c in curves]\n",
    "ax1.legend(curves, labels, loc=0)\n",
    "plt.tight_layout()\n",
    "plt.title('HYBRID (aux)')\n",
    "plt.savefig('HYBRID_aux_out' + str(epochs) + 'epochs.png')\n",
    "plt.show()\n",
    "\n",
    "# main output\n",
    "loss_train = histdict['main_output_loss']\n",
    "loss_test = histdict['val_main_output_loss']\n",
    "xplot = list(range(len(loss_train)))\n",
    "\n",
    "fig = plt.figure(num=3, figsize=(8,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "train = ax1.plot(xplot,np.sqrt(loss_train),'b-', label='Train', linewidth=4)\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Training Loss (RMSE)')\n",
    "for tl in ax1.get_yticklabels():\n",
    "    tl.set_color('b')\n",
    "ax2 = ax1.twinx()\n",
    "test = ax2.plot(xplot,np.sqrt(loss_test),'r-',label='Test',linewidth=4)\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Validation Loss (RMSE)')\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('r')\n",
    "curves = train + test\n",
    "labels = [c.get_label() for c in curves]\n",
    "ax1.legend(curves, labels, loc=0)\n",
    "plt.tight_layout()\n",
    "plt.title('HYBRID (main)')\n",
    "plt.savefig('HYBRID_main_out' + str(epochs) + 'epochs.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
