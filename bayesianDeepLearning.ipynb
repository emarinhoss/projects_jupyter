{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Deep Learning (BDL)\n",
    "[From this blog](https://towardsdatascience.com/building-a-bayesian-deep-learning-classifier-ece1845bc09)\n",
    "\n",
    "## What is Bayesian deep learning?\n",
    "Bayesian statistics is a theory in the field of statistics in which the evidence about the true state of the world is expressed in terms of degrees of belief. **The combination of Bayesian statistics and deep learning in practice means including uncertainty in your deep learning model predictions.**\n",
    "\n",
    "## What is uncertainty?\n",
    "Uncertainty is the state of having limited knowledge where it is impossible to exactly describe the existing state, a future outcome, or more than one possible outcome. As it pertains to deep learning and classification, uncertainty also includes ambiguity; uncertainty about human definitions and concepts, not an objective fact of nature.\n",
    "\n",
    "### Types of uncertainty\n",
    "* **Epistemic uncertainty** captures our ignorance about which model generated our collected data. This uncertainty can be explained away given enough data, and is often referred to as **model uncertainty**. Epistemic uncertainty is really important to model for:\n",
    "    * Safety-critical applications, because epistemic uncertainty is required to understand examples which are different from training data,\n",
    "    * Small datasets where the training data is sparse.\n",
    "    \n",
    "* **Aleatoric uncertainty** captures our uncertainty with respect to information which our data cannot explain. For example, aleatoric uncertainty in images can be attributed to occlusions (because cameras can’t see through objects). It can be explained away with the ability to observe all explanatory variables with increasing precision. Aleatoric uncertainty is very important to model for:\n",
    "    * Large data situations, where epistemic uncertainty is mostly explained away,\n",
    "    * Real-time applications, because we can form aleatoric models as a deterministic function of the input data, without expensive Monte Carlo sampling.\n",
    "    * We can actually divide aleatoric into two further sub-categories:\n",
    "        * **Data-dependant or Heteroscedastic** uncertainty is aleatoric uncertainty which depends on the input data and is predicted as a model output.\n",
    "        * **Task-dependant or Homoscedastic** uncertainty is aleatoric uncertainty which is not dependant on the input data. It is not a model output, rather it is a quantity which stays constant for all input data and varies between different tasks.\n",
    "        \n",
    "### Why is uncertainty important?\n",
    "In machine learning, we are trying to create approximate representations of the real world. Popular deep learning models created today produce a point estimate but not an uncertainty value. Understanding if your model is under-confident or falsely over-confident can help you reason about your model and your dataset.\n",
    "\n",
    "**Note: In a classification problem, the softmax output gives you a probability value for each class, but this is not the same as uncertainty. The softmax probability is the probability that an input is a given class relative to the other classes. Because the probability is relative to the other classes, it does not help explain the model’s overall confidence.**\n",
    "\n",
    "### Why is Aleatoric uncertainty important?\n",
    "**Aleatoric uncertainty is important in cases where parts of the observation space have higher noise levels than others.** \n",
    "* For example, aleatoric uncertainty played a role in the first fatality involving a self driving car. Tesla has said that during this incident, the car’s autopilot failed to recognize the white truck against a bright sky. An image segmentation classifier that is able to predict aleatoric uncertainty would recognize that this particular area of the image was difficult to interpret and predicted a high uncertainty. In the case of the Tesla incident, although the car’s radar could “see” the truck, the radar data was inconsistent with the image classifier data and the car’s path planner ultimately ignored the radar data (radar data is known to be noisy). If the image classifier had included a high uncertainty with its prediction, the path planner would have known to ignore the image classifier prediction and use the radar data instead.\n",
    "\n",
    "### Why is Epistemic uncertainty important?\n",
    "**Epistemic uncertainty is important because it identifies situations the model was never trained to understand because the situations were not in the training data.**\n",
    "* Machine learning engineers hope our models generalize well to situations that are different from the training data; however, in safety critical applications of deep learning hope is not enough. High epistemic uncertainty is a red flag that a model is much more likely to make inaccurate predictions and when this occurs in safety critical applications, the model should not be trusted.\n",
    "\n",
    "* Epistemic uncertainty is also helpful for exploring your dataset. In one case, researchers trained a neural network to recognize tanks hidden in trees versus trees without tanks. After training, the network performed incredibly well on the training set and the test set. The only problem was that all of the images of the tanks were taken on cloudy days and all of the images without tanks were taken on a sunny day. The classifier had actually learned to identify sunny versus cloudy days. Whoops.\n",
    "\n",
    "### Calculating aleatoric uncertainty\n",
    "**Aleatoric uncertainty is a function of the input data.** Therefore, a deep learning model can learn to predict aleatoric uncertainty by using a modified loss function. For a classification task, instead of only predicting the softmax values, the Bayesian deep learning model will have two outputs, the softmax values and the input variance. Teaching the model to predict aleatoric variance is an example of unsupervised learning because the model doesn’t have variance labels to learn from. Below is the standard categorical cross entropy loss function and a function to calculate the Bayesian categorical cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from tensorflow.contrib import distributions\n",
    "\n",
    "# standard categorical cross entropy\n",
    "# N data points, C classes\n",
    "# true - true values. Shape: (N, C)\n",
    "# pred - predicted values. Shape: (N, C)\n",
    "# returns - loss (N)\n",
    "def categorical_cross_entropy(true, pred):\n",
    "\treturn np.sum(true * np.log(pred), axis=1)\n",
    "\n",
    "# Bayesian categorical cross entropy.\n",
    "# N data points, C classes, T monte carlo simulations\n",
    "# true - true values. Shape: (N, C)\n",
    "# pred_var - predicted logit values and variance. Shape: (N, C + 1)\n",
    "# returns - loss (N,)\n",
    "def bayesian_categorical_crossentropy(T, num_classes):\n",
    "  def bayesian_categorical_crossentropy_internal(true, pred_var):\n",
    "    # shape: (N,)\n",
    "    std = K.sqrt(pred_var[:, num_classes:])\n",
    "    # shape: (N,)\n",
    "    variance = pred_var[:, num_classes]\n",
    "    variance_depressor = K.exp(variance) - K.ones_like(variance)\n",
    "    # shape: (N, C)\n",
    "    pred = pred_var[:, 0:num_classes]\n",
    "    # shape: (N,)\n",
    "    undistorted_loss = K.categorical_crossentropy(pred, true, from_logits=True)\n",
    "    # shape: (T,)\n",
    "    iterable = K.variable(np.ones(T))\n",
    "    dist = distributions.Normal(loc=K.zeros_like(std), scale=std)\n",
    "    monte_carlo_results = K.map_fn(gaussian_categorical_crossentropy(true, pred, dist, undistorted_loss, num_classes), iterable, name='monte_carlo_results')\n",
    "    \n",
    "    variance_loss = K.mean(monte_carlo_results, axis=0) * undistorted_loss\n",
    "    \n",
    "    return variance_loss + undistorted_loss + variance_depressor\n",
    "  \n",
    "  return bayesian_categorical_crossentropy_internal\n",
    "\n",
    "# for a single monte carlo simulation, \n",
    "#   calculate categorical_crossentropy of \n",
    "#   predicted logit values plus gaussian \n",
    "#   noise vs true values.\n",
    "# true - true values. Shape: (N, C)\n",
    "# pred - predicted logit values. Shape: (N, C)\n",
    "# dist - normal distribution to sample from. Shape: (N, C)\n",
    "# undistorted_loss - the crossentropy loss without variance distortion. Shape: (N,)\n",
    "# num_classes - the number of classes. C\n",
    "# returns - total differences for all classes (N,)\n",
    "def gaussian_categorical_crossentropy(true, pred, dist, undistorted_loss, num_classes):\n",
    "  def map_fn(i):\n",
    "    std_samples = K.transpose(dist.sample(num_classes))\n",
    "    distorted_loss = K.categorical_crossentropy(pred + std_samples, true, from_logits=True)\n",
    "    diff = undistorted_loss - distorted_loss\n",
    "    return -K.elu(diff)\n",
    "  return map_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating epistemic uncertainty\n",
    "One way of modeling epistemic uncertainty is using Monte Carlo dropout sampling (a type of variational inference) at test time. For a full explanation of why dropout can model uncertainty check out this blog and this white paper. In practice, Monte Carlo dropout sampling means including dropout in your model and running your model multiple times with dropout turned on at test time to create a distribution of outcomes. You can then calculate the predictive entropy (the average amount of information contained in the predictive distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model - the trained classifier(C classes) \n",
    "# where the last layer applies softmax\n",
    "# X_data - a list of input data(size N)\n",
    "# T - the number of monte carlo simulations to run\n",
    "def montecarlo_prediction(model, X_data, T):\n",
    "    #shape: (T, N, C)\n",
    "    predictions = np.array([model.predict(X_data) for _ in range(T)])\n",
    "    \n",
    "    # shape: (N, C)\n",
    "    prediction_probabilities = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # shape: (N)\n",
    "    prediction_variances = predictive_entropy(prediction_probabilities)\n",
    "    return (prediction_probabilities, prediction_variances)\n",
    "\n",
    "# prob - prediction probability for each class(C). Shape: (N, C)\n",
    "# returns - Shape: (N)\n",
    "def predictive_entropy(prob):\n",
    "    return -1 * np.sum(np.log(prob) * prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, RepeatVector\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "# Take a mean of the results of a TimeDistributed layer.\n",
    "# Applying TimeDistributedMean()(TimeDistributed(T)(x)) to an\n",
    "# input of shape (None, ...) returns output of same size.\n",
    "class TimeDistributedMean(Layer):\n",
    "    def build(self, input_shape):\n",
    "        super(TimeDistributedMean, self).build(input_shape)\n",
    "        \n",
    "    # input shape (None, T, ...)\n",
    "    # output shape (None, ...)\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],) + input_shape[2:]\n",
    "    \n",
    "    def call(self, x):\n",
    "        return K.mean(x, axis=1)\n",
    "\n",
    "\n",
    "# Apply the predictive entropy function for input with C classes. \n",
    "# Input of shape (None, C, ...) returns output with shape (None, ...)\n",
    "# Input should be predictive means for the C classes.\n",
    "# In the case of a single classification, output will be (None,).\n",
    "\n",
    "class PredictiveEntropy(Layer):\n",
    "    def build(self, input_shape):\n",
    "        super(PredictiveEntropy, self).build(input_shape)\n",
    "        \n",
    "    # input shape (None, C, ...)\n",
    "    # output shape (None, ...)\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],)\n",
    "    \n",
    "    # x - prediction probability for each class(C)\n",
    "    def call(self, x):\n",
    "        return -1 * K.sum(K.log(x) * x, axis=1)\n",
    "    \n",
    "def create_epistemic_uncertainty_model(checkpoint, epistemic_monte_carlo_simulations):\n",
    "    model = load_saved_model(checkpoint)\n",
    "    inpt = Input(shape=(model.input_shape[1:]))\n",
    "    x = RepeatVector(epistemic_monte_carlo_simulations)(inpt)\n",
    "    # Keras TimeDistributed can only handle a single output from a model :(\n",
    "    # and we technically only need the softmax outputs.\n",
    "    hacked_model = Model(inputs=model.inputs, outputs=model.outputs[1])\n",
    "    x = TimeDistributed(hacked_model, name='epistemic_monte_carlo')(x)\n",
    "    # predictive probabilities for each class\n",
    "    softmax_mean = TimeDistributedMean(name='epistemic_softmax_mean')(x)\n",
    "    variance = PredictiveEntropy(name='epistemic_variance')(softmax_mean)\n",
    "    epistemic_model = Model(inputs=inpt, outputs=[variance, softmax_mean])\n",
    "    \n",
    "    return epistemic_model\n",
    "\n",
    "# 1. Load the model\n",
    "# 2. compile the model\n",
    "# 3. Set learning phase to train\n",
    "# 4. predict\n",
    "def predict():\n",
    "    model = create_epistemic_uncertainty_model('model.ckpt', 100)\n",
    "    model.compile(...)\n",
    "    \n",
    "    # set learning phase to 1 so that Dropout is on. In keras master you can set this\n",
    "    # on the TimeDistributed layer\n",
    "    K.set_learning_phase(1)\n",
    "    \n",
    "    epistemic_predictions = model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note: Epistemic uncertainty is not used to train the model. It is only calculated at test time (but during a training phase) when evaluating test/real world examples. This is different than aleatoric uncertainty, which is predicted as part of the training process. Also, in my experience, it is easier to produce reasonable epistemic uncertainty predictions than aleatoric uncertainty predictions.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Bayesian deep learning classifier\n",
    "Besides the code above, training a Bayesian deep learning classifier to predict uncertainty doesn’t require much additional code beyond what is typically used to train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet50(input_shape):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = ResNet50(include_top=False, input_tensor=input_tensor)\n",
    "    # freeze encoder layers to prevent over fitting\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    output_tensor = Flatten()(base_model.output)\n",
    "    return Model(inputs=input_tensor, outputs=output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, I used the frozen convolutional layers from Resnet50 with the weights for ImageNet to encode the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bayesian_model(encoder, input_shape, output_classes):\n",
    "    encoder_model = resnet50(input_shape)\n",
    "    input_tensor = Input(shape=encoder_model.output_shape[1:])\n",
    "    x = BatchNormalization(name='post_encoder')(input_tensor)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(500, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    logits = Dense(output_classes)(x)\n",
    "    variance_pre = Dense(1)(x)\n",
    "    variance = Activation('softplus', name='variance')(variance_pre)\n",
    "    logits_variance = concatenate([logits, variance], name='logits_variance')\n",
    "    softmax_output = Activation('softmax', name='softmax_output')(logits)\n",
    "    model = Model(inputs=input_tensor, outputs=[logits_variance,softmax_output])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def encoder_min_input_size(encoder):\n",
    "    if encoder == 'resnet50':\n",
    "        return (197, 197)\n",
    "    else:\n",
    "        raise ValueError('Unexpected encoder model ' + encoder + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainable part of my model is two sets of BatchNormalization, Dropout, Dense, and relu layers on top of the ResNet50 output. The logits and variance are calculated using separate Dense layers. Note that the variance layer applies a softplus activation function to ensure the model always predicts variance values greater than zero. The logit and variance layers are then recombined for the aleatoric loss function and the softmax is calculated using just the logit layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder: resnet50\n",
      "batch_size: 32\n",
      "epochs: 1\n",
      "dataset: cifar10\n",
      "monte_carlo_simulations: 100\n",
      "Unpickling file batch_data/resnet50_cifar10/augment-train.p\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/esousa/projects/batch_data/resnet50_cifar10/augment-train.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-34716795e2f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mmin_image_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_min_input_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_train_batch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mmin_image_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_image_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/bnn/data.py\u001b[0m in \u001b[0;36mtest_train_batch_data\u001b[0;34m(dataset, encoder, is_debug, augment_data)\u001b[0m\n\u001b[1;32m     55\u001b[0m                         \u001b[0mtest_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/test.p\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_pickle_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_debug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/bnn/util.py\u001b[0m in \u001b[0;36mopen_pickle_file\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unpickling file \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0mfull_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/esousa/projects/batch_data/resnet50_cifar10/augment-train.p'"
     ]
    }
   ],
   "source": [
    "#!/bin/python \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#project_path, x = os.path.split(os.path.dirname(os.path.realpath(__file__)))\n",
    "#sys.path.append(project_path)\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "\n",
    "from bnn.model import create_bayesian_model, encoder_min_input_size\n",
    "from bnn.loss_equations import bayesian_categorical_crossentropy\n",
    "from bnn.util import isAWS, upload_s3, stop_instance, BayesianConfig\n",
    "from bnn.data import test_train_batch_data\n",
    "\n",
    "#flags = tf.app.flags\n",
    "#FLAGS = flags.FLAGS\n",
    "\n",
    "dataset = 'cifar10'\n",
    "encoder = 'resnet50'\n",
    "epochs = 1\n",
    "monte_carlo_simulations = 100\n",
    "batch_size = 32\n",
    "debug = False\n",
    "verbose = 0\n",
    "stop = True\n",
    "min_delta = 0.005\n",
    "patience = 20\n",
    "\n",
    "\n",
    "config = BayesianConfig(encoder, dataset, batch_size, epochs, monte_carlo_simulations)\n",
    "config.info()\n",
    "min_image_size = encoder_min_input_size(encoder)\n",
    "    \n",
    "((x_train, y_train), (x_test, y_test)) = test_train_batch_data(dataset, encoder, debug, augment_data=True)\n",
    "   \n",
    "min_image_size = list(min_image_size)\n",
    "min_image_size.append(3)\n",
    "num_classes = y_train.shape[-1]\n",
    "    \n",
    "model = create_bayesian_model(encoder, min_image_size, num_classes)\n",
    "    \n",
    "if debug:\n",
    "    print(model.summary())\n",
    "    callbacks = None\n",
    "else:\n",
    "    callbacks = [ModelCheckpoint(config.model_file(), verbose=verbose, save_best_only=True),\n",
    "                 CSVLogger(config.csv_log_file()),\n",
    "                 EarlyStopping(monitor='val_logits_variance_loss', min_delta=min_delta, patience=patience, verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting model train process.\")\n",
    "model.fit(x_train, \n",
    "          {'logits_variance':y_train, 'softmax_output':y_train}, \n",
    "          callbacks=callbacks,\n",
    "          verbose=FLAGS.verbose,\n",
    "          epochs=FLAGS.epochs,\n",
    "          batch_size=FLAGS.batch_size,\n",
    "          validation_data=(x_test, {'logits_variance':y_test, 'softmax_output':y_test}))\n",
    "\n",
    "print(\"Finished training model.\")\n",
    "if isAWS() and FLAGS.debug == False:\n",
    "    upload_s3(config.model_file())\n",
    "    upload_s3(config.csv_log_file())\n",
    "    if isAWS() and FLAGS.stop:\n",
    "        stop_instance()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
