{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence from AZ, Udemy\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "### What is Reinforcement Learning?\n",
    "\n",
    "Environment ----> State, reward/punishment ----> Agent ----> Action ----> Environment(Loop)\n",
    "\n",
    "*e.g.: training a dog, You give the dog a command, if he obeys the command you gve it a treat, if he doesn't he gets no treats. The dog will \"learn\" that in order to get a treat it needs to obey.*\n",
    "\n",
    "In AI you can give a +1 for rewards, or -1 for punishments (digital rewards).\n",
    "\n",
    "[Simple Reinforcement Learning with Tensorflow](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)\n",
    "\n",
    "[Reinforcement Learning I: Introduction](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.7692)\n",
    "\n",
    "### The Bellman Equation\n",
    "* necessary condition for optimality associated with the mathematical optimization method known as dynamic programming.\n",
    "* It writes the value of a decision problem at a certain point in time in terms of the payoff from some initial choices and the value of the remaining decision problem that results from those initial choices.\n",
    "* This breaks a dynamic optimization problem into a sequence of simpler subproblems, as Bellman's “principle of optimality” prescribes.\n",
    "\n",
    "**Concepts:** \n",
    "* s - State\n",
    "* a - Action\n",
    "* R - Reward\n",
    "* $\\gamma$ - Discounting\n",
    "\n",
    "$$ V(s) = max_a(R(s,a) + \\gamma V(s')) $$\n",
    "\n",
    "* V(s) - value of a state\n",
    "* s' - the next state\n",
    "\n",
    "[The Theory of Dynamic Programming](The Theory of Dynamic Programming)\n",
    "\n",
    "### The \"Plan\"\n",
    "\n",
    "* Treasure map of what to do depending of the state that you are in\n",
    "\n",
    "### Markov Decision Processes (MDP)\n",
    "\n",
    "* **Deterministic search**: given a particular input, will always produce the same output, with the underlying machine always passing through the same sequence of states. Deterministic algorithms are by far the most studied and familiar kind of algorithm, as well as one of the most practical, since they can be run on real machines efficiently.\n",
    "* **Non-deterministic search**: even for the same input, can exhibit different behaviors on different runs, often used to find an approximation to a solution, when the exact solution would be too costly to obtain using a deterministic one.\n",
    "    * concurrent algorithm can perform differently on different runs due to a race condition\n",
    "    * probabilistic algorithm's behaviors depends on a random number generator\n",
    "\n",
    "*A stochastic process has the Markov property if the conditional probability distribution future state of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it. A process with this property is called a Markov process. -Wikipedia*\n",
    "\n",
    "**Markov decision processes (MDPs)** provide a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.\n",
    "\n",
    "$$ V(s) = max_a(R(s,a) + \\gamma \\sum_{s'}P(s,a,s') V(s')) $$\n",
    "\n",
    "[A Survey of Applications of Markov Decision Processes](http://www.cs.uml.edu/ecg/uploads/AIfall14/MDPApplications3.pdf)\n",
    "\n",
    "### Policy vs Plan\n",
    "\n",
    "### \"Living Penalty\"\n",
    "\n",
    "* Reward that is given throughout the game no matter where he goes\n",
    "* Only gets this reward when an action is taken (incentive to finish the game faster)\n",
    "\n",
    "### Q-Learning \n",
    "\n",
    "* The quality of an action (if you're in specific state, there are multiple actions that can be taken, but not all are equal\n",
    "\n",
    "$$ Q(s,a) = R(s,a) + \\gamma\\sum_{s'} P(s,a,s')V(s') $$\n",
    "$$ Q(s,a) = R(s,a) + \\gamma\\sum_{s'} P(s,a,s')max_{a'}(Q(s',a)) $$\n",
    "\n",
    "[Markov Decision Processes: Concepts and Algorithms](https://pdfs.semanticscholar.org/968b/ab782e52faf0f7957ca0f38b9e9078454afe.pdf)\n",
    "\n",
    "### Temporal Difference\n",
    "\n",
    "* for simplicity we will represent\n",
    "$$ Q(s,a) = R(s,a) + \\gamma max_{a'}(Q(s',a)) $$\n",
    "* The temporal difference is defined as : \n",
    "$$ TD(a,s) = R(s,a) + \\gamma max_{a'}(Q(s',a)) - Q(s,a) $$\n",
    "* Has there been a change in Q?\n",
    "\n",
    "* How to update the Q equation over time\n",
    "$$ Q_t(s,a) = Q_{t-1}(s,a) + \\alpha TD_t(a,s) $$\n",
    "* Rewrite the equation above\n",
    "$$ Q_t(s,a) = Q_{t-1}(s,a) + \\alpha\\left(R(s,a) + \\gamma max_{a'}Q(s',a')-Q_{t-1}(s,a) \\right) $$\n",
    "\n",
    "[Learning to Predict by the Methods of Temporal Differences](https://link.springer.com/article/10.1007/BF00115009)\n",
    "\n",
    "\n",
    "### Q-Learning Visualization\n",
    "[AI at Berkeley](http://ai.berkeley.edu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Selection Policies\n",
    "* $\\epsilon$-greedy: selects he best option all the time, except $\\epsilon$ percent of the time it pics a random action\n",
    "* $\\epsilon$-soft $(1-\\epsilon)$: opposite of the above, \n",
    "* Softmax: uses the softmax function to determine the most probable/rewarding action to take\n",
    "\n",
    "The selection is governed by Exploration vs Exploitation.\n",
    "\n",
    "[Adaptive $\\epsilon$-greedy Exploration in Reinforcement Learning Based on Value Differences by Michel Tokic](http://tokic.com/www/tokicm/publikationen/papers/AdaptiveEpsilonGreedyExploration.pdf)\n",
    "\n",
    "The epsilon value is adopted depending on the state that the agent in on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!python Self_Driving_Car/map.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
