{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting with XGBoost\n",
    "\n",
    "__What is XGBoost?__\n",
    "\n",
    "* Optimized\tgradient-boosting machine learning library\n",
    "* Originally written in\tC++\n",
    "* Has APIs in several languages: Python, R, Scala, Julia, Java\n",
    "\n",
    "__What\tmakes\tXGBoost\tso\tpopular?__\n",
    "\n",
    "* Speed\tand\tperformance\n",
    "* Core algorithm is\tparallelizable \n",
    "* Consistently outperforms single-algorithm\tmethods\n",
    "* State-of-the-art performance in many ML tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.852000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sousae/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "dir1 = '/disk1/sousae/Classes/udemy_machineLearning_A-Z/Part10_Model_Selection_Boosting/'\n",
    "class_data = pd.read_csv(dir1+'Churn_Modelling.csv')\n",
    "\n",
    "X, y = class_data.iloc[:, 3:13].values, class_data.iloc[:, 13].values\n",
    "\n",
    "# Encoding categorical data\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\n",
    "\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "xg_cl.fit(X_train, y_train)\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Decision trees as base learners\n",
    "\n",
    "* Base learner - Individual learning algorithm in an ensemble algorithm\n",
    "* Composed of a series of binary questions\n",
    "* Predictions happen at the \"leaves\" of the tree\n",
    "\n",
    "__Individual decision trees tend to overfit.__\n",
    "    - low bias\n",
    "    - high variance\n",
    "    \n",
    "#### CART:\tClassification\tand\tRegression\tTrees\n",
    "\n",
    "* Each leaf\talways contains a real-valued score\n",
    "* Can later be converted into categories\n",
    "\n",
    "\n",
    "#### Decision trees\n",
    "Your task in this exercise is to make a simple decision tree using scikit-learn's DecisionTreeClassifier on the breast cancer dataset that comes pre-loaded with scikit-learn.\n",
    "\n",
    "This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign).\n",
    "\n",
    "We've preloaded the dataset of samples (measurements) into X and the target values per tumor into y. Now, you have to split the complete dataset into training and testing sets, and then train a DecisionTreeClassifier. You'll specify a parameter called max_depth. Many other parameters can be modified within this model, and you can check all of them out here.\n",
    "\n",
    "```python \n",
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Boosting?\n",
    "\n",
    "* Not a specific machine learning algorithm\n",
    "* Concept that can be applied to a set of machine learning models (_\"Meta-algorithm\"_)\n",
    "* Ensemble meta-algorithm used to convert many weak learners into a strong learner\n",
    "\n",
    "#### Weak learners and strong learners\n",
    "\n",
    "__Weak\tlearner__:\n",
    "* ML algorithm that is slightly\tbetter than\tchance\n",
    "    - Example: Decision tree whose predictions are slightly\tbetter\tthan 50%\n",
    "\n",
    "_Boosting converts a collection of weak\tlearners into a\tstrong learner._\n",
    "\n",
    "__Strong learner__:\t\n",
    "* Any algorithm\tthat can be\ttuned to achieve good performance\n",
    "\n",
    "#### How boosting is accomplished?\n",
    "\n",
    "* Iteratively learning a set of\tweak models\ton subsets of the data\n",
    "* Weighing each weak prediction\taccording to each weak learner's performance\n",
    "* Combine the weighted predictions to obtain a single weighted prediction that is much better than the individual predictions themselves!\n",
    "\n",
    "#### Cross-validation\tin\tXGBoost\texample\n",
    "```python\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "class_data = pd.read_csv(\"classification_data.csv\")\n",
    "churn_dmatrix = xgb.DMatrix(data=churn_data.iloc[:,:-1], label=churn_data.month_5_still_here)\n",
    "\n",
    "params={\"objective\":\"binary:logistic\",\"max_depth\":4}\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=4, \n",
    "                    num_boost_round=10, metrics=\"error\", as_pandas=True)\n",
    "                    \n",
    "print(\"Accuracy: %f\" %((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should I use XGBoost?\n",
    "\n",
    "* You have a large number of training samples\n",
    "* Greater than 1000\ttraining samples and less 100 features\n",
    "* The number of\tfeatures < number of training samples\n",
    "* You have a mixture of\tcategorical\tand\tnumeric\tfeatures Or\tjust numeric features\n",
    "\n",
    "#### When to NOT use XGBoost\n",
    "\n",
    "* Image recognition\n",
    "* Computer vision\n",
    "* Natural language processing and understanding problems\n",
    "* When the number of training samples is significantly smaller than the number of features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regression Review\n",
    "\n",
    "__Regression\tbasics__: Outcome is real-valued\n",
    "\n",
    "#### Common\tregression\tmetrics\n",
    "\n",
    "* Root mean squared error (RMSE)\n",
    "* Mean absolute error (MAE)\n",
    "\n",
    "### Objective (loss) functions and base learners\n",
    "```python\n",
    "boston_data\t= pd.read_csv(\"bostonhousing.csv\")\n",
    "X, y = boston_data.iloc[:,:-1],boston_data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)\n",
    "xg_reg.fit(X_train,\ty_train)\n",
    "preds =\txg_reg.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "```\n",
    "#### Objective Functions and Why We\tUse\tThem\n",
    "\n",
    "* Quantifies how far off a prediction is from the actual result\n",
    "* Measures the difference between estimated\tand\ttrue values for some collection of data\n",
    "* __Goal__:\tFind the model that yields the minimum value of the loss function\n",
    "\n",
    "#### Loss function names in xgboost:\n",
    "\n",
    "* reg:linear - use for regression problems\n",
    "* reg:logistic - use for classification\tproblems when you want just decision, not probability\n",
    "* binary:logistic - use when you want probability rather than just decision\n",
    "\n",
    "#### Base Learners and Why We Need Them\n",
    "\n",
    "* XGBoost involves creating a meta-model that is composed of many individual models\tthat combine to give a final prediction\n",
    "* Individual models\t= base learners\n",
    "* Want base learners that when combined create final prediction that is non-linear\n",
    "* Each base learner should be good at distinguishing or predicting different parts of the dataset\n",
    "* Two kinds of base learners: tree and linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        b  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_data = pd.read_csv(\"./data/BostonHousing.csv\")\n",
    "boston_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 9.749041\n"
     ]
    }
   ],
   "source": [
    "X, y = boston_data.iloc[:,:-1],boston_data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)\n",
    "xg_reg.fit(X_train,\ty_train)\n",
    "preds =\txg_reg.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
