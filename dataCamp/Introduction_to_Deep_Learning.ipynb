{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning\n",
    "\n",
    "## Part 1\n",
    "\n",
    "Imagine you work for a bank and you need to predict how many transactions each customer will make next year.\n",
    "\n",
    "### Interactions\n",
    "* Neural networks account for interactions really well\n",
    "* Deep learning uses especially powerful neural networks\n",
    "* Text\n",
    "* Images\n",
    "* Videos\n",
    "* Audio\n",
    "* Source code\n",
    "\n",
    "### Course structure\n",
    "* First two chapters focus on conceptual knowledge\n",
    "    * Debug and tune deep learning models on conventional prediction problems\n",
    "    * Lay the foundation for progressing towards modern applications\n",
    "* This will pay off in the third and fourth chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sousae/anaconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Build deep learning models with keras\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "predictors = np.loadtxt('predictors_data.csv', delimiter=',')\n",
    "n_cols = predictors.shape[1]\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(100, activation='relu')\n",
    "model.add(Dense(1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Forward propagation\n",
    "    * ABILITY TO MAKE PREDICTION\n",
    "\n",
    "#### Example case: How many bank transactions will someone do?\n",
    "* Make predictions based on:\n",
    "    * Number of children\n",
    "    * Number of existing accounts\n",
    "    \n",
    "#### Forward propagation\n",
    "* Multiply - add process\n",
    "* Dot product\n",
    "* Forward propagation for one data point at a time\n",
    "* Output is the prediction for that data point\n",
    "\n",
    "#### Forward propagation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1]\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([2, 3])\n",
    "\n",
    "weights = {'node_0': np.array([1, 1]),'node_1': np.array([-1, 1]),'output': np.array([2, -1])}\n",
    "           \n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "\n",
    "hidden_layer_values = np.array([node_0_value, node_1_value])\n",
    "\n",
    "print(hidden_layer_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Activation functions\n",
    "    * Allow model to capture non-linearities\n",
    "    * Applied to node inputs to produce node output\n",
    "    * Used to improve out neural network\n",
    "    \n",
    "### ReLU (Rectified Linear Activation)\n",
    "\n",
    "$$RELU(x) = \\Bigg\\{^{0\\quad if\\quad x < 0}_{x\\quad if\\quad x \\leq 0} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999996291\n"
     ]
    }
   ],
   "source": [
    "iput_data = np.array([-1, 2])\n",
    "\n",
    "weights = {'node_0': np.array([3, 3]),'node_1': np.array([1, 5]),'output': np.array([2, -1])}\n",
    "\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Deeper networks\n",
    "    * Mulyiple hidden layers\n",
    "\n",
    "#### Representation learning\n",
    "* Deep networks internally build representations of patterns in the data\n",
    "* Partially replace the need for feature engineering\n",
    "* Subsequent layers build increasingly sophisticated representations of raw data\n",
    "\n",
    "#### Deep learning\n",
    "* Modeler doesn’t need to specify the interactions\n",
    "* When you train the model, the neural network gets weights that find the relevant patterns to make be er predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The need for optimization\n",
    "\n",
    "#### Predictions with multiple points\n",
    "* Making accurate predictions gets harder with more points\n",
    "* At any set of weights, there are many values of the error\n",
    "    * ... corresponding to the many points we make predictions for\n",
    "    \n",
    "#### Loss function\n",
    "* Aggregates errors in predictions from many data points into single number\n",
    "* Measure of model’s predictive performance\n",
    "* Lower loss function value means a be er model\n",
    "* __Goal: Find the weights that give the lowest value for the loss function\n",
    "* Gradient descent__\n",
    "\n",
    "#### Gradient descent\n",
    "* Imagine you are in a pitch dark field\n",
    "* Want to find the lowest point\n",
    "* Feel the ground to see how it slopes\n",
    "* Take a small step downhill\n",
    "* Repeat until it is uphill in every direction\n",
    "\n",
    "#### Gradient descent steps\n",
    "* Start at random point\n",
    "* Until you are somewhere flat:\n",
    "    * Find the slope\n",
    "    * Take a step downhill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Gradient descent\n",
    "\n",
    "* If the slope is positive:\n",
    "    * Going opposite the slope means moving to lower numbers\n",
    "    * Subtract the slope from the current value\n",
    "    * Too big a step might lead us astray\n",
    "    \n",
    "* Solution: learning rate\n",
    "    * Update each weight by subtracting (__learning rate * slope__)\n",
    "\n",
    "#### Slope calculation example\n",
    "    3 --- 2 ---> 6 : Actul target value = 10\n",
    "    \n",
    "    * To calculate the slope for a weight, need to multiply:\n",
    "    \n",
    "        * Slope of the loss function w.r.t value at the node we feed into\n",
    "            * Slope of mean-squared loss function w.r.t prediction: 2*Error = 2*(-4)\n",
    "            \n",
    "        * The value of the node that feeds into our weight\n",
    "            * 2*(-4)*(3) = -24\n",
    "            * If learning rate is 0.01, the new weight would be 2-0.01(-24)=2.24\n",
    "            \n",
    "        * Slope of the activation function w.r.t value we feed into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([1, 2])\n",
    "input_data = np.array([3, 4])\n",
    "target = 6\n",
    "learning_rate = 0.01\n",
    "preds = (weights * input_data).sum()\n",
    "error = preds - target\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 40])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = 2 * input_data * error\n",
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n"
     ]
    }
   ],
   "source": [
    "weights_updated = weights - learning_rate * gradient\n",
    "preds_updated = (weights_updated * input_data).sum()\n",
    "error_updated = preds_updated - target\n",
    "print(error_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Making multiple updates to weights\n",
    "\n",
    "You're now going to make multiple updates so you can dramatically improve your model weights, and see how the predictions improve with each update.\n",
    "\n",
    "To keep your code clean, there is a pre-loaded _get_slope()_ function that takes _input_data_, _target_, and _weights_ as arguments. There is also a _get_mse()_ function that takes the same arguments. The _input_data_, _target_, and _weights_ have been pre-loaded.\n",
    "\n",
    "This network does not have any hidden layers, and it goes directly from the input (with 3 nodes) to an output node. Note that weights is a single array.\n",
    "\n",
    "We have also pre-loaded _matplotlib.pyplot_, and the error history will be plotted after you have done your gradient descent steps.\n",
    "\n",
    "```python \n",
    "n_updates = 20\n",
    "mse_hist = []\n",
    "\n",
    "# Iterate over the number of updates\n",
    "for i in range(n_updates):\n",
    "    # Calculate the slope: slope\n",
    "    slope = get_slope(input_data, target, weights)\n",
    "    \n",
    "    # Update the weights: weights\n",
    "    weights = weights - 0.01 * slope\n",
    "    \n",
    "    # Calculate mse with new weights: mse\n",
    "    mse = get_mse(input_data, target, weights)\n",
    "    \n",
    "    # Append the mse to mse_hist\n",
    "    mse_hist.append(mse)\n",
    "\n",
    "# Plot the mse history\n",
    "plt.plot(mse_hist)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Backpropagation\n",
    "\n",
    "* Allows gradient descent to update all weights in neural network (by getting gradients for all weights)\n",
    "* Comes from chain rule of calculus\n",
    "* Important to understand the process, but you will generally use a library that implements this\n",
    "\n",
    "#### Backpropagation process\n",
    "\n",
    "* Trying to estimate the slope of the loss function w.r.t each weight\n",
    "* Do forward propagation to calculate predictions and errors\n",
    "\n",
    "* Go back one layer at a time\n",
    "* Gradients for weight is product of:\n",
    "    1. Node value feeding into that weight\n",
    "    2. Slope of loss function w.r.t node it feeds into\n",
    "    3. Slope of activation function at the node it feeds into\n",
    "\n",
    "\n",
    "* Need to also keep track of the slopes of the loss function w.r.t node values\n",
    "* Slope of node values are the sum of the slopes for all weights that come out of them\n",
    "\n",
    "#### Backpropagation: Recap\n",
    "\n",
    "* Start at some random set of weights\n",
    "* Use forward propagation to make a prediction\n",
    "* Use backward propagation to calculate the slope of the loss function w.r.t each weight\n",
    "* Multiply that slope by the learning rate, and subtract from the current weights\n",
    "* Keep going with that cycle until we get to a flat part\n",
    "\n",
    "#### Stochastic gradient descent\n",
    "\n",
    "* It is common to calculate slopes on only a subset of the data (‘batch’)\n",
    "* Use a different batch of data to calculate the next update\n",
    "* Start over from the beginning once all data is used\n",
    "* Each time through the training data is called an epoch\n",
    "* When slopes are calculated on one batch at a time: stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Creating a keras model\n",
    "\n",
    "#### Model building steps\n",
    "\n",
    "* Specify Architecture\n",
    "* Compile\n",
    "* Fit\n",
    "* Predict\n",
    "\n",
    "#### Model specification\n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "predictors = np.loadtxt('predictors_data.csv', delimiter=',')\n",
    "\n",
    "n_cols = predictors.shape[1]\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "```\n",
    "\n",
    "#### Compiling a model\n",
    "\n",
    "* Specify the optimizer\n",
    "* Many options and mathematically complex\n",
    "    * “Adam” is usually a good choice\n",
    "* Loss function\n",
    "    * “mean_squared_error” common for regression\n",
    "    \n",
    "```python\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "```\n",
    "\n",
    "#### Fitting a model\n",
    "\n",
    "* Applying backpropagation and gradient descent with your data to update the weights\n",
    "* Scaling data before fitting can ease optimization\n",
    "\n",
    "```python\n",
    "model.fit(predictors, target)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Classification models\n",
    "\n",
    "* ‘categorical_crossentropy’ loss function\n",
    "* Similar to log loss: Lower is better\n",
    "* Add metrics = [‘accuracy’] to compile step for easy-to-understand diagnostics\n",
    "* Output layer has separate node for each possible outcome, and uses ‘softmax’ activation\n",
    "\n",
    "```python\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "data = pd.read_csv('basketball_shot_log.csv')\n",
    "\n",
    "predictors = data.drop(['shot_result'], axis=1).as_matrix()\n",
    "target = to_categorical(data.shot_result)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(predictors, target)\n",
    "```\n",
    "\n",
    "#### Using models\n",
    "\n",
    "* Save\n",
    "* Reload\n",
    "* Make predictions\n",
    "\n",
    "```python\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('model_file.h5')\n",
    "\n",
    "my_model = load_model('my_model.h5')\n",
    "\n",
    "predictions = my_model.predict(data_to_predict_with)\n",
    "probability_true = predictions[:,1]\n",
    "\n",
    "# Verifying model structure\n",
    "my_model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Understanding model optimization\n",
    "\n",
    "#### Why optimization is hard\n",
    "* Simultaneously optimizing 1000s of parameters with complex relationships\n",
    "* Updates may not improve model meaningfully\n",
    "* Updates too small (if learning rate is low) or too large (if learning rate is high)\n",
    "\n",
    "```python\n",
    "def get_new_model(input_shape = input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return(model)\n",
    "    \n",
    "lr_to_test = [.000001, 0.01, 1]\n",
    "\n",
    "# loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    model = get_new_model()\n",
    "    my_optimizer = SGD(lr=lr)\n",
    "    model.compile(optimizer = my_optimizer, \n",
    "                  loss = 'categorical_crossentropy')\n",
    "\n",
    "model.fit(predictors, target)\n",
    "```\n",
    "#### The dying neuron problem\n",
    "\n",
    "* when a node starts getting always negative inputs (relu activation)\n",
    "    * it may continue to only getting negative inputs\n",
    "* Contributes nothing to the model (\"Dead\" neuron)    \n",
    "\n",
    "#### Vanishing gradients\n",
    "\n",
    "* Occurs when many layers have very small slopes (e.g. due to being on flat part of tanh curve)\n",
    "* In deep networks, updates to backprop were close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation in deep learning\n",
    "\n",
    "* Commonly use validation split rather than cross-validation\n",
    "* Deep learning widely used on large datasets\n",
    "* Single validation score is based on large amount of data, and is reliable\n",
    "* Repeated training from cross-validation would take long time\n",
    "\n",
    "```python\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(predictors, target, validation_split=0.3)\n",
    "\n",
    "# ====== Early Stopping ======= #\n",
    "from keras.callbacks import EarlyStopping\n",
    "# patience tells how many epoch the model can go without improving\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "model.fit(predictors, target, validation_split=0.3, epochs=20,\n",
    "          callbacks = [early_stopping_monitor])\n",
    "```\n",
    "\n",
    "### Example: Adding layers to a network\n",
    "\n",
    "You've seen how to experiment with wider networks. In this exercise, you'll try a deeper network (more hidden layers).\n",
    "\n",
    "Once again, you have a baseline model called model_1 as a starting point. It has 1 hidden layer, with 50 units. You can see a summary of that model's structure printed out. You will create a similar network with 3 hidden layers (still keeping 50 units in each layer).\n",
    "\n",
    "This will again take a moment to fit both models, so you'll need to wait a few seconds to see the results after you run your code.\n",
    "\n",
    "```python\n",
    "# The input shape to use in the first hidden layer\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first, second, and third hidden layers\n",
    "model_2.add(Dense(50, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(50, activation='relu'))\n",
    "model_2.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model 1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model 2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking about model capacity\n",
    "\n",
    "* Pay attention to overfitting\n",
    "\n",
    "#### Workflow for optimizing model capacity\n",
    "* Start with a small network\n",
    "* Gradually increase capacity\n",
    "* Keep increasing capacity until validation score is no longer improving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recognizing handwritten digits\n",
    "\n",
    "* MNIST dataset\n",
    "* 28 x 28 grid fla ened to 784 values for each image\n",
    "* Value in each part of array denotes darkness of that pixel\n",
    "\n",
    "\n",
    "### Example: Building your own digit recognition model\n",
    "\n",
    "You've reached the final exercise of the course - you now know everything you need to build an accurate model to recognize handwritten digits!\n",
    "\n",
    "We've already done the basic manipulation of the MNIST dataset shown in the video, so you have X and y loaded and ready to model with. Sequential and Dense from keras are also pre-imported.\n",
    "\n",
    "To add an extra challenge, we've loaded only 2500 images, rather than 60000 which you will see in some published results. Deep learning models perform better with more data, however, they also take longer to train, especially when they start becoming more complex.\n",
    "\n",
    "```python\n",
    "# Create the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "model.add(Dense(50, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Add the second hidden layer\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, validation_split=0.3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
